FER2013 ემოციების ამოცნობა

ეს პროექტი წარმოადგენს სახის ემოციების ამოცნობის ამოცანას.

მონაცემები
მონაცემთა ნაკრებში წარმოდგენილია ემოციების 7 კლასი: ["sad", "fear", "angry", "neutral", "happy", "surprise", "disgust"]. თითოეული ნიმუში არის 48x48 პიქსელის ნაცრისფერი სურათი.

 წინასწარი დამუშავება
პიქსელების მნიშვნელობები მასშტაბირდება 0-1 დიაპაზონში 255-ზე გაყოფით, რაც ხელს უწყობს მოდელის სტაბილურ სწავლებას.

იმბალანსის პრობლემა
მონაცემთა ნაკრებს აქვს კლასების არათანაბარი განაწილების პრობლემა. ამის მოსაგვარებლად გამოიყენება enhance_dataset_distribution ფუნქცია, რომელიც ახდენს ნაკლებად წარმოდგენილი კლასების გამრავლებას (upsampling) მაქსიმალურ რაოდენობამდე. გამრავლებულ სურათებს ენიჭება needs_augmentation ფლაგი.

აუგმენტაცია
ფლაგით მონიშნულ სურათებზე ხორციელდება სიკაშკაშის შემთხვევითი მოდიფიკაცია 0.9-1.1 კოეფიციენტით. ეს ცვლილება საკმარისად მცირეა იმისთვის, რომ არ დაამახინჯოს ემოციის გამომხატველი მახასიათებლები, მაგრამ საკმარისია მრავალფეროვნების შესაქმნელად.

მონაცემთა ჩატვირთვა
მონაცემები იტვირთება batch-ებად DataLoader-ის საშუალებით, რომელიც უზრუნველყოფს ეფექტურ პარალელურ დამუშავებას 2 worker-ის გამოყენებით. სატრენინგო მონაცემები შემთხვევითად ირევა (shuffle=True), ხოლო ვალიდაციისა და ტესტის მონაცემები - არა.

basic მოდელის სტრუქტურა
გამოყენებულია მარტივი კონვოლუციური ნეირონული ქსელი (CNN), რომელიც შედგება სამი კონვოლუციური ფენისგან მზარდი ფილტრების რაოდენობით (32, 64, 128). თითოეული კონვოლუციური ფენის შემდეგ გამოიყენება MaxPooling და ReLU აქტივაცია. ბოლოს ორი სრულად დაკავშირებული ფენა ახორციელებს 7 ემოციის კლასიფიკაციას.

მოდელი ტრენინგდება Adam ოპტიმიზატორით (learning rate = 0.001) და CrossEntropy loss ფუნქციით. სწავლება მიმდინარეობდა 10 ეპოქის განმავლობაში, batch ზომით 256.

სწავლების დინამიკა
სატრენინგო მონაცემებზე მოდელმა აჩვენა სტაბილური გაუმჯობესება:
თუმცა, ვალიდაციის შედეგები მიუთითებს overfitting-ის პრობლემაზე:
f1_score_Angry	0.39781
f1_score_Disgust	0.47059
f1_score_Fear	0.38375
f1_score_Happy	0.73958
f1_score_Neutral	0.4501
f1_score_Sad	0.41558
f1_score_Surprise	0.67114
training_accuracy	0.81739
training_loss	0.52997
validation_accuracy	0.52421
validation_loss	1.62998

https://wandb.ai/ttora21-free-university-of-tbilisi-/Emotion_Recognition/runs/dhn8f49y

EnhancedNeuralNet
1. ინიციალიზაცია (__init__ მეთოდი)
კონსტრუქტორი ინიციალიზებს სხვადასხვა ფენებს ქსელში:
-  არქიტექტურა იყენებს ოთხ კონვოლუციურ ბლოკს, რომლებიც ზრდიან გამომავალი არხების რაოდენობას.
-  სრულად დაკავშირებული (FC) ქსელი, რომელიც ამოღებულ ინფორმაციას ბრტყელს ხდის და კლასიფიცირებს შვიდ კატეგორიაში.
2. _create_conv_block მეთოდი
ეს მეთოდი ქმნის ფენების თანმიმდევრობას, რომელიც მოიცავს:
- 2D კონვოლუციურ ფენას, რომელიც აღჭურვილია ბატჩის ნორმალიზაციით.
- ReLU აქტივაციის ფუნქციას, რაც უზრუნველყოფს არალინეარობას.
- Dropout რეგულარიზაციისთვის, რათა თავიდან იქნას აცილებული ზედმეტი სწავლება.
- დამატებით მაქსიმალური პულინგის ფენას.

3. _create_fc_block მეთოდი
ეს მეთოდი ქმნის სრულად დაკავშირებულ ფენას, რომელიც მოიცავს:
- ლინეარულ ტრანსფორმაციას.
- ბატჩის ნორმალიზაციას, რომელიც უზრუნველყოფს სწავლების პროცესის სტაბილიზაციას.
- ReLU აქტივაციას.
- Dropout რეგულარიზაციისთვის.

4. forward მეთოდი
forward მეთოდი განსაზღვრავს, როგორ გადის შემავალი ტენზორი ქსელის საშუალებით:
- ჯერ გადის ყველა ფუნქციების ამოღების ფენაზე.
- შემდეგ გადის მოწინავე ფუნქციების დამუშავების ფენებზე.
- ბოლოს, გამოსავალი კლასიფიკატორში შედის საბოლოო პროგნოზების წარმოებისთვის.
https://wandb.ai/ttora21-free-university-of-tbilisi-/Emotion_Recognition/runs/sf3yheoz?nw=nwuserttora21



AdvancedNetwork
1. CustomBlock 
CustomBlock არის კასტომიზებული ნეირონული ბლოკი, რომელიც:
- მოიცავს ორ კონვოლუციურ ფენას, რომლებიც იყენებენ ბატჩის ნორმალიზაციას და ReLU აქტივაციის ფუნქციას.
- ითვალისწინებს "skip connection" ფუნქციონალობას, რაც საშუალებას აძლევს ქსელს უმჯობესად ისწავლოს.
- იყენებს Dropout რეგულარიზაციას, თუ drop_prob განაკვეთი უფრო დიდია 0-ზე.

2. AdvancedNetwork 
AdvancedNetwork არის მთავარი ნეირონული ქსელი, რომელიც:
- იწყება ერთი კონვოლუციური ბლოკით
- შეიცავს რამდენიმე CustomBlock-ს, რაც ზრდის არხების რაოდენობას და ახორციელებს სივრცულ შემცირებას.
- იყენებს ადაპტიურ საშუალო პულინგს, რაც უზრუნველყოფს საბოლოო რუკის 1x1 ზომას.
- კლასიფიკაციის ფენა, რომელიც შედგება სრულად დაკავშირებული ფენების სერიისგან.

EnhancedNeuralNet და AdvancedNetwork ტრენინგის პარამეტრების განსაზღვრა
- batch_size: 256
- learning_rate: 0.001
- num_epochs: 40
https://wandb.ai/ttora21-free-university-of-tbilisi-/Emotion_Recognition/runs/47qp516w


EmotionRecognitionNet მოდელის არქიტექტურა
1. Feature Layers:
   - მოდელი იწყება ოთხი კონვოლუციური ბლოკით, რომლებიც განლაგებულია `nn.ModuleList`-ში:
        - create_conv_block
       - კონვოლუციური ფენა
       - ბაჩ ნორმალიზაცია
       - ReLU აქტივაციის ფუნქცია
       - Dropout(`nn.Dropout2d`)
       - Max Pooling ზოგიერთ ბლოკში გამოიყენება, რათა შემცირდეს სივრცული განზომილება.

2. Processing Layers
   - მოდელი შეიცავს ორი დამატებით კონვოლუციურ ბლოკს
     -არ გამოიყენება Max Pooling, რაც საშუალებას აძლევა შენარჩუნდეს მეტი ინფორმაცია.

3. Squeeze-and-Excitation ბლოკი
   - ეს ბლოკი კონცენტრირდება იმ მახასიათებლებზე, რომლებიც ყველაზე მნიშვნელოვანი აღმოჩნდება
     - `fc1` და `fc2` - ხელმისაწვდომი მახასიათებლების მოდულირება, რაც არგუმენტებად გამოიყენება.

4. Classifier
   - ბოლო ნაწილი არის კლასიფიკაციის ფენა, რომელიც მოიცავს:
     - Flatten-მახასიათებლების გადატანა ერთ განზომილებაში.
     - რამდენიმე სრულად დაკავშირებული ფენა (`_create_fc_block`), რომლებიც მოიცავს:
       - ბაჩ ნორმალიზაციადა ReLU აქტივაცია
       - Dropout 
     
 ძირითადი მახასიათებლები:
- Dropout ზედმეტი სწავლის თავიდან ასაცილებლად.
- Batch Normalization სტაბილურობისა და ეფექტურობის გაზრდისთვის.
- Squeeze-and-Excitation ყურადღების კონცენტრირება მნიშვნელოვან მახასიათებლებზე.
https://wandb.ai/ttora21-free-university-of-tbilisi-/Emotion_Recognition


საბოლოოდ გავტესტე ტესტ სეტზე და ავაგე confusion მატრიცები.
არჩევანი შევაჩერე advanced მოდელზე.
ბევრად მაღალი აქვს როგორც სწავლების, ასევე ვალიდაციის სიზუსტე.
F1-score-ები, რაც უკეთ ზომავს ზოგად შესრულებას (განსაკუთრებით როცა კლასი დაბალანსებული არ არის), უკეთესია.
მიუხედავად იმისა, რომ Validation Loss-ი მაღალია, საერთო პროგნოზის სიზუსტე მაინც უკეთესია.

